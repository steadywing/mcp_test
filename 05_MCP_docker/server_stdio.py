from pathlib import Path 

from mcp.server.fastmcp import FastMCP
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI

VECTORDB_PATH = Path(__file__).parents[0] / "my-vectordb"
#-------------------------------------------------------------------
# 0. VectorDB ì¸ìŠ¤í„´ìŠ¤ ë§Œë“¤ê¸°
#-------------------------------------------------------------------
def load_retriever():
    print("ğŸ‘€ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘.....")
    # ì„ë² ë”© ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    embedding_model = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",  # ë˜ëŠ” multilingual: "BAAI/bge-m3"
        model_kwargs={"device": "cpu"},  # or "cuda" / "mps" / "cpu"
        encode_kwargs={"normalize_embeddings": True}
    )
    print("âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!")

    # FAISS ë¡œë“œ ë° Retriever ìƒì„±
    vectordb = FAISS.load_local(VECTORDB_PATH, embedding_model, allow_dangerous_deserialization=True)
    retriever = vectordb.as_retriever()

    return retriever

retriever = load_retriever()
#-------------------------------------------------------------------
# 1. MCP ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
#-------------------------------------------------------------------
mcp = FastMCP(
    name="rag-mcp-server"
)

#-------------------------------------------------------------------
# 2. ë„êµ¬ ë“±ë¡
#-------------------------------------------------------------------
@mcp.tool()
def rag_qa(question: str) -> str:
    """ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ RAG QA. ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    results = retriever.invoke(question)

    if not results:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    return "\n".join([doc.page_content for doc in results])

#-------------------------------------------------------------------
# 3. MCP ì„œë²„ ì‹¤í–‰
#-------------------------------------------------------------------
if __name__ == "__main__":
    mcp.run(transport="stdio")